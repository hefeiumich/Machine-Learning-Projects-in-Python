{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pairs \n",
    "Can you identify question pairs that have the same intent?\n",
    "<br> https://www.kaggle.com/c/quora-question-pairs\n",
    "<br> The goal of this project is to use natural language processing tools and advanced techniques to classify whether \n",
    "question pairs are duplicates or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the necessary packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import wordnet\n",
    "import numpy\n",
    "import nltk \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hefei/Documents/Kaggle/Quora/Code/old\n"
     ]
    }
   ],
   "source": [
    "# Check directory\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change working directory\n",
    "os.chdir('/Users/hefei/Documents/Kaggle/Quora/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Done data loading\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print (\"Load data...\")\n",
    "dftrain = pd.read_csv(\"./train.csv\",encoding='utf-8').fillna(\"\")\n",
    "dftest = pd.read_csv(\"./test.csv\",encoding='utf-8').fillna(\"\")\n",
    "print (\"Done data loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done question1 preprocess train\n",
      "done question2 preprocess train\n",
      "done question1 preprocess test\n",
      "done question2 preprocess test\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing the data\n",
    "\n",
    "# The Lemmatizing part of codes are from second place solution of CrowdFlower Kaggle Project\n",
    "# The authors are: Mikhail Trofimov, Stanislav Semenov, Dmitry Altukhov\n",
    "#########################\n",
    "###  Lemmatizing part ###\n",
    "#########################\n",
    "\n",
    "logging.info('Lemmatizing')\n",
    "toker = TreebankWordTokenizer()\n",
    "lemmer = wordnet.WordNetLemmatizer()\n",
    "\n",
    "def text_preprocessor(x):\n",
    "    '''\n",
    "    Get one string and clean\\lemm it\n",
    "    '''\n",
    "    tmp = unicode(x)\n",
    "    tmp = tmp.lower().replace('blu-ray', 'bluray').replace('wi-fi', 'wifi')\n",
    "    x_cleaned = tmp.replace('/', ' ').replace('-', ' ').replace('\"', '')\n",
    "    tokens = toker.tokenize(x_cleaned)\n",
    "    return \" \".join([lemmer.lemmatize(z) for z in tokens])\n",
    "\n",
    "# lemm question1\n",
    "dftrain['question1']  = dftrain['question1'].apply(text_preprocessor)\n",
    "print (\"done question1 preprocess train\")\n",
    "dftest[ 'question1']  = dftest['question1'].apply(text_preprocessor)\n",
    "print (\"done question2 preprocess train\")\n",
    "# lemm question2\n",
    "dftrain['question2']  = dftrain['question2'].apply(text_preprocessor)\n",
    "print (\"done question1 preprocess test\")\n",
    "dftest[ 'question2']  = dftest['question2'].apply(text_preprocessor)\n",
    "print (\"done question2 preprocess test\")\n",
    "\n",
    "y_train = dftrain['is_duplicate'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start feature extraction for train\n",
      "done feature extraction for train\n",
      "done feature extraction test\n"
     ]
    }
   ],
   "source": [
    "# Feature selection\n",
    "# 1. common words shared ratio of question1 \n",
    "# 2. common words shared ratio of question2\n",
    "# common words mean the common words between question1 and question2\n",
    "# ratio = number of common words/number of question1 (or question2)\n",
    "# 3. the absolute value of difference of total numbers between two questions\n",
    "# 4. cosine similarity between two questions \n",
    "\n",
    "N = dftrain.shape[0]\n",
    "Ntest = dftest.shape[0]\n",
    "m = 4 # number of features planned to extract \n",
    "X_train = numpy.zeros((N,m))\n",
    "X_test = numpy.zeros((Ntest,m))\n",
    "# First column: common words shared normalized by the total number of question1 \n",
    "# Second column: common words shared normalized by the total number of question 2\n",
    "# Third column: absolute value of total number difference between question1 and question2\n",
    "# Fourth column: cosine similarity between two questions\n",
    "print (\"start feature extraction for train\")\n",
    "for i in range(0,N):\n",
    "# Count the common words ratio shared between two questions for train data \n",
    "# Count the total number difference of words for train data\n",
    "    x1 = dftrain.question1[i].split()\n",
    "    list1 = map(lambda x: x.lower(), x1)\n",
    "    x2 = dftrain.question2[i].split()\n",
    "    list2 = map(lambda x: x.lower(), x2)\n",
    "    set2 = set(list2)\n",
    "    f = lambda x:x in set2\n",
    "    x3 = filter(f, list1)\n",
    "    cwr1 = len(x3)*1.0/(len(list1)+1)\n",
    "    X_train[i,0] = cwr1\n",
    "    cwr2 = len(x3)*1.0/(len(list2)+1)\n",
    "    X_train[i,1] = cwr2\n",
    "    wdiff = abs(len(x2)-len(x1))\n",
    "    X_train[i,2] = wdiff\n",
    "     \n",
    "# Count the cosine similarity between two questions \n",
    "    documents = (dftrain.question1[i],dftrain.question2[i])\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "    x0 = cosine_similarity(tfidf_matrix[0:1],tfidf_matrix)[0]\n",
    "    X_train[i,3] = x0[1]\n",
    "\n",
    "    \n",
    "print (\"done feature extraction for train\")\n",
    "\n",
    "for i in range(0,N):\n",
    "# Count the common words ratio shared between two questions for train data \n",
    "# Count the total number difference of words for train data\n",
    "    x1 = dftest.question1[i].split()\n",
    "    list1 = map(lambda x: x.lower(), x1)\n",
    "    x2 = dftest.question2[i].split()\n",
    "    list2 = map(lambda x: x.lower(), x2)\n",
    "    set2 = set(list2)\n",
    "    f = lambda x:x in set2\n",
    "    x3 = filter(f, list1)\n",
    "    cwr1 = len(x3)*1.0/(len(list1)+1)\n",
    "    X_test[i,0] = cwr1\n",
    "    cwr2 = len(x3)*1.0/(len(list2)+1)\n",
    "    X_test[i,1] = cwr2\n",
    "    wdiff = abs(len(x2)-len(x1))\n",
    "    X_test[i,2] = wdiff\n",
    "     \n",
    "# Count the cosine similarity between two questions \n",
    "    documents = (dftest.question1[i],dftest.question2[i])\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "    x0 = cosine_similarity(tfidf_matrix[0:1],tfidf_matrix)[0]\n",
    "    X_test[i,3] = x0[1]\n",
    "\n",
    "print (\"done feature extraction test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save X_train and X_test for future use\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(X_train)\n",
    "df.to_csv('xtrain.csv')\n",
    "\n",
    "df = pd.DataFrame(X_test)\n",
    "df.to_csv('xtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with sgd\n"
     ]
    }
   ],
   "source": [
    "# Build models using X_train and predict using X_test\n",
    "# stochastic gradient descent classification\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"log\").fit(X_train, y_train)\n",
    "y_predict_sgd = clf.predict_proba(X_test)[:,0]  \n",
    "print \"done with sgd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save output \n",
    "import pandas as pd \n",
    "ysub = pd.read_csv(\"./sample_submission.csv\")\n",
    "ysub['is_duplicate'] = y_predict_sgd\n",
    "submission = pd.DataFrame(ysub)\n",
    "submission.to_csv(\"./final_solution_sgd.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
